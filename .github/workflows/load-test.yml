name: Load Testing

on:
  # Run on demand
  workflow_dispatch:
    inputs:
      target_env:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      duration:
        description: 'Test duration (e.g., 5m, 10m)'
        required: true
        default: '5m'
      users:
        description: 'Number of virtual users'
        required: true
        default: '100'

  # Run on schedule (weekly performance regression test)
  schedule:
    - cron: '0 2 * * 1'  # Every Monday at 2 AM UTC

  # Run after successful deployment
  workflow_run:
    workflows: ["Deploy Backend API"]
    types:
      - completed

jobs:
  load-test-k6:
    name: k6 Load Test
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set target URL
        id: set-url
        run: |
          if [ "${{ github.event.inputs.target_env }}" == "production" ]; then
            echo "BASE_URL=https://api.flowshare.com" >> $GITHUB_OUTPUT
          else
            echo "BASE_URL=https://staging-api.flowshare.com" >> $GITHUB_OUTPUT
          fi

      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Run k6 load test
        env:
          BASE_URL: ${{ steps.set-url.outputs.BASE_URL }}
          TEST_TOKEN: ${{ secrets.LOAD_TEST_TOKEN }}
          TENANT_ID: ${{ secrets.LOAD_TEST_TENANT_ID }}
        run: |
          cd backend/load_tests
          mkdir -p results

          k6 run \
            --out json=results/k6-results.json \
            --summary-export=results/k6-summary.json \
            k6_test.js

      - name: Upload k6 results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: k6-results
          path: backend/load_tests/results/

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = JSON.parse(fs.readFileSync('backend/load_tests/results/k6-summary.json'));

            const comment = `
            ## Load Test Results

            **Test Configuration:**
            - Duration: ${{ github.event.inputs.duration || '5m' }}
            - Virtual Users: ${{ github.event.inputs.users || '100' }}
            - Target: ${process.env.BASE_URL}

            **Performance Metrics:**
            - Total Requests: ${summary.metrics.http_reqs.values.count}
            - Failed Requests: ${(summary.metrics.http_req_failed.values.rate * 100).toFixed(2)}%
            - Avg Response Time: ${summary.metrics.http_req_duration.values.avg.toFixed(2)}ms
            - P95 Response Time: ${summary.metrics.http_req_duration.values['p(95)'].toFixed(2)}ms
            - P99 Response Time: ${summary.metrics.http_req_duration.values['p(99)'].toFixed(2)}ms

            **Thresholds:**
            ${summary.metrics.http_req_duration.values['p(95)'] < 2000 ? '✅' : '❌'} P95 < 2000ms
            ${summary.metrics.http_req_failed.values.rate < 0.01 ? '✅' : '❌'} Error Rate < 1%
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Check performance thresholds
        run: |
          cd backend/load_tests/results
          # Parse k6 summary and check thresholds
          python3 << 'EOF'
          import json
          import sys

          with open('k6-summary.json') as f:
              summary = json.load(f)

          metrics = summary['metrics']

          # Check thresholds
          p95 = metrics['http_req_duration']['values']['p(95)']
          error_rate = metrics['http_req_failed']['values']['rate']

          print(f"P95 Latency: {p95:.2f}ms (threshold: <2000ms)")
          print(f"Error Rate: {error_rate*100:.2f}% (threshold: <1%)")

          if p95 > 2000:
              print("❌ FAIL: P95 latency exceeds 2000ms")
              sys.exit(1)

          if error_rate > 0.01:
              print("❌ FAIL: Error rate exceeds 1%")
              sys.exit(1)

          print("✅ PASS: All thresholds met")
          EOF

  load-test-locust:
    name: Locust Load Test
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          cd backend/load_tests
          pip install -r requirements.txt

      - name: Set target URL
        id: set-url
        run: |
          if [ "${{ github.event.inputs.target_env }}" == "production" ]; then
            echo "BASE_URL=https://api.flowshare.com" >> $GITHUB_OUTPUT
          else
            echo "BASE_URL=https://staging-api.flowshare.com" >> $GITHUB_OUTPUT
          fi

      - name: Run Locust load test
        env:
          BASE_URL: ${{ steps.set-url.outputs.BASE_URL }}
        run: |
          cd backend/load_tests
          mkdir -p results

          locust -f locustfile.py \
            --host=${{ steps.set-url.outputs.BASE_URL }} \
            --users ${{ github.event.inputs.users || '100' }} \
            --spawn-rate 10 \
            --run-time ${{ github.event.inputs.duration || '5m' }} \
            --headless \
            --csv=results/locust \
            --html=results/locust-report.html \
            --loglevel INFO

      - name: Upload Locust results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: locust-results
          path: backend/load_tests/results/

      - name: Analyze results
        run: |
          cd backend/load_tests/results
          echo "=== Locust Test Results ==="
          cat locust_stats.csv | head -20

  performance-regression-check:
    name: Performance Regression Check
    needs: [load-test-k6]
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
      - name: Download current results
        uses: actions/download-artifact@v4
        with:
          name: k6-results
          path: current/

      - name: Download baseline results
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: load-test.yml
          branch: main
          name: k6-results
          path: baseline/
        continue-on-error: true

      - name: Compare performance
        run: |
          python3 << 'EOF'
          import json
          import sys
          from pathlib import Path

          # Load current results
          with open('current/k6-summary.json') as f:
              current = json.load(f)

          current_p95 = current['metrics']['http_req_duration']['values']['p(95)']

          # Try to load baseline
          baseline_file = Path('baseline/k6-summary.json')
          if baseline_file.exists():
              with open(baseline_file) as f:
                  baseline = json.load(f)

              baseline_p95 = baseline['metrics']['http_req_duration']['values']['p(95)']

              # Calculate regression
              regression_pct = ((current_p95 - baseline_p95) / baseline_p95) * 100

              print(f"Current P95: {current_p95:.2f}ms")
              print(f"Baseline P95: {baseline_p95:.2f}ms")
              print(f"Regression: {regression_pct:+.1f}%")

              # Fail if more than 20% regression
              if regression_pct > 20:
                  print(f"❌ FAIL: Performance regression > 20%")
                  sys.exit(1)
              else:
                  print(f"✅ PASS: Performance within acceptable range")
          else:
              print("No baseline found, skipping regression check")
          EOF
